# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains data of clients(32951) with 20 input variables collected through marketing campaigns held by banking institution and we seek to predict, if client subscribes to a term deposit or not i.e.,target variable(y) with a value yes/no. 

## Best performing model 

The best performing model was AutoML with accuracy of 0.9154738177206015 by Voting Ensemble algorithm.

## Scikit-learn Pipeline

**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

### Pipeline architecture of HyperDrive Experiment

Create a workspace and experiment objects to start building a hyperdrive pipeline in jupyter notebook.

**1. Create a Compute cluster :** Training a model requires virtual machine in which experiment is run through creating compute cluster with required vm priority - Low priority, vm type - CPU, vm size - Standard_D2_v2 and max_nodes - 4.

**2. Set up train.py script :** This python script file is used to run the hyperdrive experiment that includes custom coded Logistic Regression model using sklearn. 
* **Data Import :**
Create tabular dataset of bankmarketing_csv file by importing of data using azureml TabularDatasetFactory class.

* **Clean and Encode data :**
A clean_data function is used to replace or clean the missing values in dataset and is subjected to one hot encoding where categorical values are converted to numbers.

* **Split Data :**
Split of data into train and test subsets is done using train_test_split function with specified random state of split(random_state=42) and size of test set(test_size=0.33).

* **Script arguments :**
LogisticRegression class is used to regularise by specifying parameters like *Regularization strength* and *Maximum number of iterations*.  

**3. Create HyperDrive configuration :** Creating a configuration for hyperdrive run to execute experiment with specified parameters like maximum total no.of runs to create and execute concurrently, name of the primary metric and primary metric goal is defined along with following hyperparameters:

* **Parameter sampler :**
Specifying parameter sampler using RandomParameterSampling class that enables random sampling over a hyperparameter search space from a set of discrete or continuous values(C and max_iter) 

* **Policy :**
Specifies early termintaion policy for early stopping with required amount of evaluation interval, slack factor and delay_evaluation.

* **Estimator :**
SKLearn class is used to create a estimator to use with train.py script that specifies source directory, compute target, vm size, vm priority, entry script file path.

**4. Submit Hyperdrive run :** Submission of hyperdriveconfig to run the experiment. View progress of run through RunDetails widget.

**5. Retrieve best run :** 
Use get_best_run_by_primary_metric method for hyperdrive run to choose best hyperparameters of model and retrieve metrics of run.

**6. Save model :**
Use the joblib import to save the trained model that creates a new file with specified name for model in outputs directory.


## Benefits of Random Parameter Sampler

* Random parameter sampling supports both discrete and continuous hyperparameter values.
* Helps to identify low performing runs and thereby helps in early termination.
* Low bias in random sampling as hyperparameter values are randomly selected from the defined search space and have equal probability of being selected.
* choice function helps to sample from only specified set of values.
* uniform function helps to maintain uniform distribution of samples taken.

## Benefits of Bandit Policy 

* It is a early termination policy that terminates low performing runs which improves computation for existing runs.
* It terminates runs if primary metric is not within specified slack factor in comparison to the best performing run.
* This policy is based on slack factor and evaluation interval.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The hyperdrive optimized logistic regression model produced an accuracy of 0.91024462 and AutoML model is with accuracy of 0.91547381 from Voting Ensemble algorithm. The difference in accuracy is 0.00522919. 

The differences in architecture are hyperdrive run requires manual tuning of hyperparameters by running through different hyperparameter values and choose the best hyperparameter that outperforms the other. Whereas, automl run tunes the hyperparameters automatically that results in producing best hyperparameters for that run and also automates model selection.

Yes, there was a slight difference in accuracy. As AutoML model processes different machine learning algorithms and Voting Ensemble being ensemble algorithm that combines predictions of multiple models could be the reason for difference in accuracy and better model performance.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up

![capture](images/screenshot(76).png)
